---
title: "Workshop 1, Financial Modeling and Programming"
author: "Alberto Dorantes, Ph.D."
date: "Nov 13, 2023"

abstract: "This is an INDIVIDUAL wordokshop. In this workshop we learn about logistic regression applied to fundamental analysis in Finance. In addition, we practice data management programming skills using a big panel-dataset of historical financial statement variables." 

format: 
  html:
    toc: true
    toc-title: Content    
    toc-location: left
    toc-float: true
    theme: united
    highlight-style: zenburn
    number-sections: true
    fontsize: 0.9em
    html-math-method: katex
    
knitr:
  opts_chunk: 
    warning: false
    message: false
---

```{r global_options}
#| include: false 
#| warning: false
#| message: false
#| fig-width: 12
#| fig-height: 8

#knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
#                      warning=FALSE, message=FALSE)
```

```{r, include=FALSE}
# Funci√≥n para desplegar en formato de dinero $
din<-function(x) {
  paste0("$", formatC(as.numeric(x), format="f", digits=2, big.mark=","))
}
# To avoid scientific notation for numbers: 
options(scipen=999)

```

```{r}
#| echo: false
library(kableExtra)
```

# Introduction

In this workshop we practice data management with financial historical data, and review the logit regression model and its application to fundamental analysis.

We will work with a real dataset of all historical financial variables of ALL US public firms that belong to the NYSE and the NASDAQ exchanges.

We will use a logit model to examine whether some financial ratios are related to the probability that future stock return (1 year later) is higher than the industry future return (1 year later).

We will learn basic programming skills for the required data management and data modeling.

# Review of logit regression and its applications in Finance

The logit model is one type of non-linear regression model where the dependent variable is binary. The logistic or logit model is used to examine the relationship between one or more quantitative variables and the probability of an event happening (1=the event happens; 0 otherwise). For example, a bank that gives loans to businesses might be very interested in knowing which are the factors/variables/characteristics of firms that are more related to loan defaults. If the bank understands these factors, then it can improve its decisions about which firms deserve a loan, and minimize the losses due to loan defaults.

In this workshop, we define the event to be whether a firm in a specific quarter has higher stock return compared to median returns of the firms within its industry. If the stock return is higher than the median return, then we codify the binary variable equal to 1; 0 otherwise.

Then, in this case, the dependent variable of the regression is the binary variable with 1 if the stock outperforms the median of its industry and 0 otherwise. The independent or explanatory variables can be any financial indicator/ratio/variable that we believe is related to the likelihood of a stock to beat the industry in the near future.

We can define this logistic model using the following mathematical function. Imagine that Y is the binary dependent variable, then the probability that the event happens (Event=1) can be defined as:

$$
Prob(Event=1)=f(X_{1},X_{2},...,X_{n})
$$

The binary variable Event can be either 1 or 0, but the probability of Event=1 is a continuous value from 0 to 1. The function is a non-linear function defined as follows:

$$
Prob(Event=1)=\frac{1}{1+e^{-\left(b_{0}+b_{1}X_{1}+b_{2}X_{2}+...+b_{n}X_{n}\right)}}
$$

As we can see, the argument of the exponential function is actually a traditional regression equation. We can re-express this equation as follows:

$$
Y=b_{0}+b_{1}X_{1}+b_{2}X_{2}+...+b_{n}X_{n}
$$ 
Now we use Y in the original non-linear function:

$$
Prob(Event=1)=\frac{1}{1+e^{-Y}}
$$

This is a **non-linear function** since the value of the function does not move in a linear way with a change in the value of one independent variable X.

Let's work with an example of a model with only 1 independent variable $X_1$. Imagine that $X_1$ is the variable **earnings per share (eps)** of a firm, and the event is that the firm beats the industry. Then, let's do a simple example with specific values for $b_0$, $b_1$, and a range of values for $eps$ from -1 to 1 jumping by 0.1:

```{r}
# The seq function creates a numeric vector. We specify the fist, last and the jumps:
eps=seq(from=-1,to=1,by=0.1)
eps
# This vector has 21 values for x1 including the zero

# I define b0=-0.5 and b1=10:
b0=-0.5
b1=10

# I define a temporal variable Y to be the regression equation:
Y = b0 + b1*eps
# Since eps is a vector, then R performs a row-wise calculation using the equation for all values of eps

# Now I create a vector with the values of the function according to the equation of the probability:
prob = 1 / (1 + exp(-(Y)) )
# I display the probability values of the function for all values of eps:
prob

# Finally, I plot the function values for all values of x1:
plot(x=eps,y=prob,type="line")
            
```

Here we can see that function is **not linear** with changes in eps. There is a specific range of values for eps close to 0 when the probability that the firm beats the industry increases very fast up to a value of about 0.3 where the probability grows very slow with any more increase in eps.

The interpretations of the magnitude of the coefficients $b_0$ and $b_1$ in logistic regression is not quite the same as the case of multiple regression. However, the interpretations of the sign of the coefficient (positive or negative) and the level of significance (pvalue) are the same as in the case of multiple regression model. What we can say up to know is that if $b_1$ is positive and significant (its p-value\<0.05), then it means that the variable, in this case, **eps** is significantly and positively related to the probability that a firm return outperform its industry median return.

Before going to the interpretation of the magnitude of a coefficient, here is a quick explanation of how the logistic regression works and how it is estimated in any specialized software (such as R).

Let's continue with the same event, which is that the firm return beats its industry median (in other words, that the firm return is higher than the median return of its industry). Then:

p = probability that the firm beats its industry (Event=1); or that the event happens.

(1-p) = probability that the firm DOES NOT beat its industry (Event=0); or that the event does not happen.

To have a dependent variable that can get any numeric value from a negative value to a positive value, we can do the following mathematical transformation with these probabilities:

$$
Y=log\left(\frac{p}{1-p}\right)
$$

The $\left(\frac{p}{1-p}\right)$ is called the **odds ratio**:

$$
ODDSRATIO=\left(\frac{p}{1-p}\right)
$$

The odds ratio is the ratio of the probability of the event happening to the probability of the event NOT happening. Since p can have a value from 0 to 1, then the possible values of ODDSRATIO can be from 0 (when p=0) to infinity (when p=1). Since we want a variable that can have values from any negative to any positive value, then we can apply the logarithmic function, and then the range of this log will be from any negative value to any positive value.

Now that we have a transformed variable Y (the log of ODDSRATIO) that uses the probability p, then we can use this variable as the dependent variable for our regression model:

$$
Y=log\left(\frac{p}{1-p}\right)=b_{0}+b_{1}X_{1}
$$

This mathematical *trick* help us to use a linear model to model a non-linear relationship!

Then, with this transformation we can estimate a linear regression model (don't worry, R estimate it), so the coefficients $b_0$ and $b_1$ values define the logarithm of the odd ratio!, not the actual probability p of the event happening!

How can we interpret the magnitude of the beta coefficients of this regression? Let's do a mathematical trick from the previous equation. We can apply the exponential function to both sides of the equation:

$$
e^{Y}=e^{log\left(\frac{p}{1-p}\right)}=e^{\left(b_{0}+b_{1}X_{1}\right)}=\left(\frac{p}{1-p}\right)=ODDSRATIO
$$

Following the rule of exponents, we can express this equation as:

$$
e^{Y}=e^{b_{0}}e^{b_{1}X_{1}}=ODDSRATIO
$$

Let's see what happens with ODDSRATIO if $X_1$ increases in 1 unit:

$$
e^{b_{0}}e^{b_{1}(X_{1}+1)}=e^{b_{0}}e^{b_{1}X_{1}}e^{b_{1}}=ODDSRATIO*(e^{b_{1}})
$$

Then, if $X_1$ increases in one unit, then the ODDSRATIO will be equal to ODDSRATIO times $e^{b_1}$. Then, $e^{b_1}$ will be the factor that indicates how many times the ODDSRATIO changes with a +1-unit change in $X_1$.

Then:

-   If $e^{b_1}$ = 1, then the ODDSRATIO does not change, meaning that there is no relationship between the variable $X_1$ and the probability of the event.

-   If $e^{b_1}$ \> 1, then the ODDSRATIO will grow by this factor, meaning that there is a positive relationship between $X_1$ and the probability of the event.

-   If $e^{b_1}$ \< 1, then the ODDSRATIO will decrease by this factor, meaning that there is a negative relationship between $X_1$ and the probability of the event.

Then, when we see the output of a logistic regression, we need to apply the exponential function to the coefficients to provide a meaningful interpretation of the magnitude of the coefficients in the model.

If we want to estimate the probability p for any value of $X_1$, we just need to do some algebraic manipulations to the previous equation:

$$
Y=log\left(\frac{p}{1-p}\right)=b_{0}+b_{1}X_{1}
$$

To get p from this equation, we can apply the exponential function to both sides:

$$
e^{Y}=\left(\frac{p}{1-p}\right)
$$

Leaving p alone, we multiply both sides times (1-p):

$$
e^{Y}\left(1-p\right)=p
$$

We continue playing with the terms to leave p alone:

$$
e^{Y}-pe^{Y}=p
$$

$$
e^{Y}=p+pe^{Y}
$$

$$
e^{Y}=p(1+e^{Y})
$$

$$
p=\frac{e^{Y}}{\left(1+e^{Y}\right)}
$$

Dividing the numerator and the denominator by $e^Y$ ;

$$
p=\frac{1}{\left(\frac{1}{e^{Y}}+1\right)}=\frac{1}{\left(e^{-Y}+1\right)}
$$

This is the same mathematical equation we had used above to illustrate the non-linear relationship between the explanatory variables and the probability of an event happening!

Fortunately, R performs all these calculations automatically when you run the predict function! So, you do not have to memorize these steps; I just tried to be *curious* to understand how the logistic model estimates predicted probabilities.

# Data collection

The dataset we will use for this workshop and for the final project is located in a public we site.

This dataset has real historical information of all US public firms listed in NASDAQ and NYSE. It has data from Q1 2000 to Q2 2023. You can download the dataset as follows.

You first need to install the packages ggplot2 and dplyr. You can do so using the right-hand side window of RStudio in the "Package" tab.

Before running the rest of the chunks, you need to save your .Rmd in your computer.

```{r}
#| message: false
#| warning: false

# To avoid scientific notation:
options(scipen=999)
#Activate the dplyr library
library(dplyr)

# Load the datasets 
uspanel <- read.csv("dataus2023.csv")
usfirms <- read.csv("firmsus2023.csv")

```

# CHALLENGE 1 - CALCULATE QUARTERLY EBIT FROM YTD EBIT

Using the uspanel dataset calculate EBIT as: revenue - cogs - sgae.

The income-statement variables in this dataset are cummulative amounts (Year-to-Date) by fiscal year. 

Since each firm can have different quarter to end the fiscal year, it is a very good idea to calculate quarterly amounts from the YTD cumulative amounts.

Remember that the YTD amount is a cumulative amount from the first fiscal quarter up to any fiscal quarter. Then, what might be the algorithm to create the quarter-amount EBIT ? 

**CHALLENGE 1 - WRITE YOUR ALGORITHM WITH YOUR OWN WORDS, AND WRITE THE R CODE TO CREATE THE QUARTERLY EBIT AMOUNT IN THE USPANEL DATASET**

# CHALLENGE 1 SOLUTION 

My algorithm has the following steps:

1 - For all firms-quarters, calculate the Year-to-date (YTD) EBIT as:

YTDEBIT = revenue - COGS - SGAE 

Since the columns revenue, COGS and SGAE are fiscal YTD amounts, then EBIT will also be calculated as fiscal YTD amount.

2 - Calculate EBITp as follows:
   + For all firm-quarters that are NOT the first fiscal quarter, calculate EBITp as equal to YTDEBIT minus its previous YTDEBIT. To know whether a quarter is the first fiscal quarter, we check whether the fiscalmonth variable has value of 3 or not.  
   + For the first fiscal quarters of all firms (when fiscalmonth is equal to 3), assign EBITp equal to YTDEBIT. 
   
Let's write the R code to implement this algorithm: 


```{r}
library(dplyr)
uspanel <- uspanel %>% 
  mutate(YTDebit = revenue - cogs - sgae, 
         ebitp = ifelse(fiscalmonth==3,YTDebit,YTDebit - lag(YTDebit)))

```

I can see the result of one firm:

```{r}
uspanel %>% select(firm,q,fiscalmonth,yearf,YTDebit,ebitp) %>% 
  filter(firm=="AAPL", yearf>=2020)
```

The values of ebitp looks ok. For the fist fiscal quarters (when fiscalmonth==3), YTDebit is equal to ebitp. For the rest of the quarters, ebitp is equal to YTDebit minus its previous YTDebit! 


# Brief descriptive statistics

We can use the dplyr package to get important descriptive statistics for the US firms. Since we have a panel data, we can select only the last fiscal quarter of 2022, and then get a list of firms and calculate important descriptive statistics about the sample.

## CHALLENGE 2 - Creating a cross-sectional dataset 

For all firms in the dataset, you have to select quarter with the last complete fiscal annual data (fiscal year=2022 and fiscalmonth=12)

If we do descriptive statistics with the panel data, the information will be repeated since we have many rows for the same company. We need to have a cross-sectional data where the information of 1 firm is only in 1 row of the dataset.

In the panel data we have 4 quarters for each year. In the US, most of the firms end the fiscal year in the Q4, however there are few firms that end the fiscal year in the other 3 quarters. To identify for each firm when the fiscal year ends, there is a column in the dataset called fiscalmonth. If the fiscalmonth=12, that means that that quarter is the end of the corresponding fiscal year.

The last complete fiscal year in the dataset is 2022 (actually, it is the most recent we have available). Then, we can select only those rows with yearF=2022 and fiscalmonth=12 to end up with the last fiscal quarter for each firm.

**CHALLENGE 2 - Write R code to create this cross-sectional dataset (name the dataset as data2022**  

# CHALLENGE 2 SOLUTION

I just select observation with yearf==2022 and fiscalmonth==12:

```{r}

data2022 <- as.data.frame(uspanel) %>%
      filter(yearf==2022, fiscalmonth==12)
nrow(data2022)
```

The us2022 dataset no has only `r nrow(data2022)` observations; one observation for each firm with the last complete fiscal information of 2022. 

# CHALLENGE 3 - PASTE THE INDUSTRY INTO THE USPANEL AND THE DATA2022 DATASETS

For the uspanel and the data2022 datasets, you have to add a column with the industry name (naics1 code). Remember that the industry along with firm name and other variables are in the firms2023 dataset.

**WRITE DE R CODE FOR THIS MERGE FOR BOTH DATASETS** You can use the merge or the left_join function. 

# CHALLENGE 3 SOLUTION

I merge the uspanel with the usfirms dataset to get the industry name:

```{r}
usfirms1 <- usfirms %>%
  mutate(firm = empresa) %>% 
  select(firm,naics1)

uspanel <- left_join(uspanel,usfirms1,by='firm')
```

I do the same for the data2022 dataset:

```{r}
data2022 <- left_join(data2022, usfirms1, by='firm')
```


# CHALLENGE 4 - DESCRIPTIVE STATISTICS OF MARKET VALUE BY INDUSTRY

Using the data2022, calculate the market value for all firms. Show a table by INDUSTRY to show the # of firms in the industry along with the Q1, median, and Q3 of market value by INDUSTRY. 

**CHALLENGE 4 - WRITE THE R CODE TO SHOW # OF FIRMS, Q1, MEDIAN AND Q3 BY INDUSTRY** 

# CHALLENGE 4 SOLUTION

Remember that values are stored in thousands ('1,000s) of US dollars.

I use dplyr to generate a column for marketcap and then use group_by to do statistics of marketcap by industry:

```{r}
data2022 <- data2022 %>%
  mutate(marketcap = originalprice * sharesoutstanding)

stats_by_industry <- data2022 %>%
  group_by(naics1) %>% 
  summarize(firms = n(),
            median_marketcap = median(marketcap, na.rm = TRUE),
            Q1_marketcap = quantile(marketcap, probs=c(0.25),na.rm=TRUE),
            Q3_marketcap = quantile(marketcap,probs=c(0.75),na.rm=TRUE)
            )
```

```{r, eval=FALSE}
stats_by_industry
```

```{r, echo=FALSE}
stats_by_industry %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```


I will interpret the information of the  Manufacturing industry:
We have `r stats_by_industry$firms[7]` Manufacturing firms in the sample. The typical Manufacturing firm size in terms of market capitalization is its median marketcap, which is \$USD `r din(1000*stats_by_industry$median_marketcap[7])`. The median is the best measure of central tendency for any financial variable when we have many firms since the distribution of these variables is always skewed to the right (there are very few big firms and many firms with a more reasonable size).


# CHALLENGE 5 - CALCULATE VARIABLES FOR THE MODEL 

Using the historical uspanel dataset, calculate:

- Market value = originalprice * sharesoutstanding
- oepsp = Operational Earnings per Share divided by price = (EBITQ/sharesoutstanding) / originalprice
- r = Annual stock return (log return) 

You can use dplyr package for these variables.
Remember that EBITQ is the Quarterly EBIT you calculated in Challenge 1.

**CHALLENGE 5: WRITE THE R CODE TO CALCULATE THESE VARIABLES**

# CHALLENGE 5 SOLUTION

```{r}
uspanel <- uspanel %>%
  arrange(firm,q) %>%
  group_by(firm) %>%
  mutate(marketvalue = originalprice * sharesoutstanding,
         oepsp = (ebitp / sharesoutstanding) / originalprice,
         r = log(adjprice) - dplyr::lag(log(adjprice),4)
  ) %>%
  ungroup()
```


# CHALLENGE 6 - WINSORIZE THE OPERATING EPSP

**CHALLENGE 6 - Apply winsorization to the oepsp variable. Use the statar package to automatically wisorize the variable and a histogram of the winsorized oepsp.** 

# CHALLENGE 6 SOLUTION

I apply winsorization to oepsp:

```{r}
library(statar)
hist(uspanel$oepsp)
uspanel$oepspw= winsorize(uspanel$oepsp)
hist(uspanel$oepspw)

```


# CHALLENGE 7 - RUN A LOGIT MODEL TO PREDICT PROBABILITY OF BEATING THE INDUSTRY

**CHALLENGE 7 - USING THE glm FUNCTION, RUN A LOGISTIC MODEL TO EXAMINE WHETHER oepspw HAS EXPLANATORY POWER TO THE PROBABILITY OF A STOCK BEATING THE MEDIAN INDUSTRY RETURN**

# CHALLENGE 7 SOLUTION


I first calculate a column in the uspanel with the median return by industry. I can use dplyr to do this:

```{r}
uspanel <- uspanel %>%
  group_by(q,naics1) %>%
  mutate(industry_ret = median(r,na.rm=TRUE)) %>%
  ungroup()
```

We can have a look to make sure that we computed the median of annual return by quarter-industry:

```{r, eval=FALSE}
uspanel %>% select(firm,q,naics1,r,industry_ret) %>%
  filter(q=="2023q2",naics1=="Industrias manufactureras") %>%
  head(10)
```

```{r, echo=FALSE}
uspanel %>% select(firm,q,naics1,r,industry_ret) %>%
  filter(q=="2023q2",naics1=="Industrias manufactureras") %>%
  head(10) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

```{r}
uspanel %>% select(firm,q,naics1,r,industry_ret) %>%
  filter(q=="2023q2",naics1=="Industrias manufactureras") %>%
  summarize(medianret= median(r,na.rm=TRUE))
```

I got the same median return for this industry in this quarter. It looks ok.

I create a binary variable with 1 if the stock return is greater than the median return of its industry; 0 otherwise:

```{r}
uspanel$rabove = ifelse(uspanel$r>uspanel$industry_ret,1,0)
# we can see how many 1's and 0's were calculated:
table(uspanel$rabove)

```

I run the logit model to examine whether oepspw is related to the probability of a stock to beat its industry return

```{r}
logitm1 <- glm(rabove ~ oepspw, data= uspanel, family="binomial",na.action=na.omit)
summary(logitm1)
coefficients = coef(logitm1)
coefficients

# The coefficients for the odd ratio interpretation are:
exp(0.1*coefficients)


```
**YOU HAVE TO INTERPRET THIS MODEL WITH YOUR WORDS**.

**LOOKING AT THE BETA COEFFICIENT OF THE EXPLANATORY VARIABLE oepspw WE CAN SEE THAT THE COEFFICIENT IS POSITIVE AND STATISTICALLY SIGNIFICANT (SINCE ITS P-VALUE IS MUCH LESS THAN 0.05). THEN, WE CAN SAY THAT OPERATING EARNINGS PER SHARE DEFLATED BY PRICE IS SIGNIFICANTLY AND POSITIVELY RELATED WITH THE PROBABILITY OF A STOCK BEATING THE industry (IN TERMS OF ANNUAL RETURN).**

**THE WAY WE INTERPRET THE MAGNITUDE OF THE COEFFICIENT IS NOT THE SAME AS IN THE CASE OF LINEAR REGRESSION. WE NEED TO GET THE EXPONENTIAL OF THE COEFFICIENT TO HAVE A BETTER INTERPRETATION. ANOTHER WAY TO UNDERSTAND WHY WE NEED TO GET THE EXPONENTIAL OF THE COEFFICIENT IS THAT THE ACTUAL DEPENDENT VARIABLE OF THE LOGIT MODEL IS THE LOGARITHM OF ODD RATIO. REMEMBER THAT THE ODD RATIO IS THE DIVISION BETWEEN THE PROBABILITY OF THE STOCK BEATING THE INDUSTRY AND THE PROBABILITY OF THE STOCK NOT BEATING THE INDUSTRY. THEN, IF WE APPLY THE EXPONENTIAL FUNCTION TO THE LOGARITHM OF THE ODDS RATIO WE GET THE ACTUAL ODDS RATIO:**

$$
Y_{i,t}=log(\frac{p}{1-p})=b_{0}+b_{1}(oepspw_{i,t})+\varepsilon_{i,t}
$$

$$
e^{Y_{i,t}}=\left(\frac{p}{1-p}\right)
$$

**SINCE THE EXPECTED VALUE OF $Y$ IS ACTUALLY THE REGRESSION EQUATION, THEN I CAN APPLY THE EXPONENTIAL EQUATION TO THE REGRESSION EQUATION AND GET TRANSFORMED BETAS THAT ARE RELATED TO THE ODDS RATIO (NOT TO THE LOG OF ODDS RATIO). THIS IS CONSISTENT WITH A PREVIOUS EXPLANATION ABOUT HOW TO INTERPRET THE MAGNITUDE OF A COEFFICIENT OF A LOGIT REGRESSION. AS I HAD EXPLAINED IN A PREVIOUS SECTION:**

**IF THE EXPLANATORY VARIABLE INCREASES IN 1 UNIT:**

$$
e^{b_{0}}e^{b_{1}(oepspw+1)}=e^{b_{0}}e^{b_{1}(oepspw)}e^{b_{1}}=ODDSRATIO*(e^{b_{1}})
$$

**THEN, $(e^{b_{1}})$ IS THE TRANSFORMED COEFFICIENT WE NEED TO ESTIMATE IN ORDER TO HAVE A BETTER INTERPRETATION OF THE REGRESSION OUTPUT. LET'S GET THE EXPONENTIAL OF THE REGRESSION COEFFICIENTS:**

**IN THE REAL WORLD, oepspw CANNOT MOVE +1.0 UNIT SINCE IT IS A RATIO. THEN, WE CAN BETTER THINK IN WHAT WOULD HAPPEN IF THE oepspw MOVES IN 0.1 UNIT:**

$$
e^{b_{0}}e^{b_{1}(oepspw+0.1)}=e^{b_{0}}e^{b_{1}(oepspw)}e^{0.1*b_{1}}=ODDSRATIO*(e^{0.1*b_{1}})
$$

```{r}
# I USE THE coef FUNCTION TO EXTRACT THE COEFFICIENT FROM THE LOGIT MODEL:
coefficients <- coef(logitm1)

coefficients

```

**NOW I CAN APPLY THE EXPONENTIAL FUNCTION TO THIS VECTOR THAT CONTAINS THE COEFFICIENTS $b_0$ AND $b_1$:**

```{r}
odd_coeffs <- exp(0.1*coefficients)
odd_coeffs
# I SAVE THE MODIFIED COEFFICIENTS:
modified_b0 = odd_coeffs[1]
modified_b1 = odd_coeffs[2]

```

**NOW THE TRANSFORMED $b_1$ IS `r odd_coeffs[2]`. WE INTERPRET THIS COEFFICIENT AS FOLLOWS:**

**IF A FIRM IMPROVES ITS oepspw BY ONE 0.1 UNIT, ITS ODDS RATIO -THE PROBABILITY TO BEAT THE industry WITH RESPECT TO THE PROBABILITY OF NOT BEATING THE industry- IN THE SAME QUARTER WILL BE `r modified_b1` TIMES HIGHER THAN THE CASE THAT THE FIRM DOES NOT IMPROVE ITS oepspw.**

**THE INTERPRETATION OF $b_0$ IS THE FOLLOWING. IF oepspw IS EQUAL TO ZERO, THEN THE TRANSFORMED COEFFICIENT $b_0$ WILL BE THE EXPECTED ODDS RATIO. IN THIS CASE, THE TRANSFORMED COEFFICIENT OF $b_0$ IS `r odd_coeffs[1]`, which is less than 1. THIS MEANS THAT THE PROBABILITY THAT THE STOCK BEATS THE industry IS LESS THAN THE PROBABLITY DOES NOT BEAT THE industry.**

**WE CAN GET A BETTER INTERPRETATION OF $b_0$ CALCULATING THE EXPECTED PROBABILITY OF BEATING THE industry FROM THE EXPECTED ODDS RATIO WHEN oepsp=0. WE NEED TO TRANSFORM FROM ODDSRATIO TO PROBABILITY:**

$$
ODDSRATIO=\frac{p}{1-p}
$$

DOING SOME ALGEBRA WE CAN EXPRESS P IN TERMS OF ODDSRATIO:

$$
(1-p)ODDSRATIO=p
$$

$$
ODDSRATIO - p(ODDSRATIO) = p
$$

$$
ODDSRATIO = p + p(ODDSRATIO) = p(1+ODDSRATIO)
$$

$$
p=\frac{ODDSRATIO}{1+ODDSRATIO}
$$

**WE HAD CALCULATED THE exp(b0), the TRANSFORMED COEFFICIENT OF $b_0$ = `r odd_coeffs[1]`, WHICH IS THE EXPECTED ODDSRATIO IF oepsp=0. THEN, THE EXPECTED PROBABILITY IF oepsp=0 WILL BE: $p=\frac{`r odd_coeffs[1]`}{1+`r odd_coeffs[1]`}$, then p=`r odd_coeffs[1]/(1+odd_coeffs[1])`.**

```{r}
# Prob of beating the industry when oepsp=0:
p = odd_coeffs[1] / (1+odd_coeffs[1])
p
```

**IN TERMS OF PROBABILITY, IF THE CURRENT ODDS-RATIO=1 (THEN p=0.5) AND IF A FIRM IMPROVES ITS oepspw BY 0.1, ITS PROBABILITY TO BEAT THE industry IS EXPECTED TO IMPROVE:**

```{r}
# If current odds-ratio of a firm = 1, its current probability to beat the industry must be 1
current_odds_ratio = 1
current_p = current_odds_ratio / (1+current_odds_ratio)
current_p

# If oepsp moves in 0.1 unit, then its new odds_ratio will be:
new_odds_ratio = current_odds_ratio * exp(0.1*b1) 
# The, the probability will increase:
new_p = new_odds_ratio / (1 + new_odds_ratio)
new_p

```

**THEN, IN THIS HYPOTHETICAL CASE, IF oepsp INCREASES IN +0.1 UNIT, THEN THE PROBABILITY TO BEAT THE INDUSTRY WILL INCREASE FROM `r current_p` TO `r new_p`.**


# Prediction with the logit model

We can use the last model to predict the probability whether the firm will beat the return of the industry 1 year in the future. For this prediction, it might be a good idea to select only the firms in the last quarter of the dataset (2022Q2), so we can predict which firm will beat the industry in 2023:

We create a dataset with only the Q2 of 2023:

```{r}
data2023 <- uspanel %>%
          select(firm,q,yearf, rabove,oepspw) %>%
          filter(q=="2023q2") %>%
        as.data.frame()
hist(data2023$oepspw)
```

Now we run the prediction using the model 2 with this new dataset:

```{r}
data2023 <- data2023 %>% 
  mutate(pred=predict(logitm1,newdata=data2023,type=c("response")) )
  
```

The type=c("response") parameter indicates to get the prediction in probability. If I do not specify this option, the prediction is calculated in the logarithm of ODDS RATIO.

Let's do this prediction:

```{r}
data2023 <- data2023 %>% 
  mutate(logodds_pred=predict(logitm1,newdata=data2023),
# Following the non-linear equation we explained, we get the predicted probability: 
         predprob = 1 / (1+exp(-logodds_pred)) )

```

The predprob and the pred columns have the same predicted probability:

```{r, eval=FALSE}
head(data2023)

```

```{r, echo=FALSE}
head(data2023) %>%
  kbl() %>%
  kable_paper("hover", full_width = F)
```

We can do a histogram to see how this predicted probability behaves:

```{r}
hist(data2023$pred,breaks=20)

```

We can also see how the predicted probability of beating the industry changes with changes in earnings per share:

```{r}
library(ggplot2)
# I use ggplot from the ggplot2 library:
ggplot(data2023, aes(x=oepspw, y=pred)) +  
  geom_point() # I specify to do a scatter plot

```

It is curious to see that the relationship between oepspw and the probability of beating the industry looks similar to a linear relationship. However, we can see a tiny curvature, indicating that the model is doing the non-linear effect, but if we had more extreme values of oepspw we could see the S-shape relationship.

To explore what would the model predict with a wider range of values for the explanatory variable, the oepspw. I will create a vector with a wide range for oepspw, and then get the predictions using the model 2

Testing what happens with hypothetical values of eps, including extreme values from -0.50 up to 0.50:

```{r}
# I create a sequence from -4 to 4 for eps:
eps1 = as.data.frame(seq(from=-0.50,to=0.50,by=0.01))

names(eps1)=c("oepspw")
# I do the prediction of these eps hypothetical values:
eps1$pred1 <- predict(logitm1,newdata=eps1,type=c("response"))

ggplot(eps1, aes(x=oepspw, y=pred1)) +  
  geom_point() # I specify to do a scatter plot


```

We see the logistic S-shape non linear relationship if we predict the probability for a wide range of values for oepspw!

# Selection of best stocks based on results of the logit model

We can sort the firms according to the predicted probability of beating the benchmark, and then select the top 5% with the highest probability:

```{r}

top100 <- data2023 %>%
     merge(usfirms1,by="firm") %>% # merge with firms1 to pull firm name and industry
     arrange(desc(pred)) %>%  # sort by probability, from highest to lowest
     #slice_head(prop=0.05) %>% # select the top 5% firms  
     head(n=100)

# Show the # of firms selected:
nrow(top100)

#Showing the first 10 top firms:
head(top100,10)

# Showing a summary by industry (Economatica) of the selected top 
top100 %>% 
   group_by(naics1) %>%
   summarize(firms = n(), mean_probability = mean(pred,na.rm=TRUE) )  
# Showing a summary by industry (NAICS1) of the selected top 5% 
top100 %>% 
   group_by(naics1) %>%
   summarize(firms = n(), mean_probability = mean(pred,na.rm=TRUE) )  

```

# Datacamp online courses and other materials

You will receive an email invitation to register in datacamp.com. Datacamp is one of the best online learning sites for Data Science applied to Business and Finance.

You will receive **free access** for the whole semester! Accept the invitation to be registered in Datacamp.

You can get official certificates from Datacamp and post them in your Linkedin profile!

It is recommended that you take/review the following chapters:

-   Chapters 1 to 3 of the course: **Introduction to tidyverse**

-   Chapter "Logistic Regression" of the course: **Credit Risk Modeling in R**

Few days ago (Oct 25, 2022), Max Kuhn and Julia Silge published an online **free book: [Tidy Modeling with R](https://www.tmwr.org/).** Max Kuhn has written R packages such as caret. I strongly recommend it!


